# Monet_Paintings_Lodia
Assignment For Generative models: Iâ€™m Something of a Painter Myself

WANDB: https://wandb.ai/llodi22-free-university-of-tbilisi-/Monet_Paintings_Lodia/table?nw=nwuserllodi22

áƒ›áƒáƒ“áƒ˜ áƒ¯áƒ”áƒ  áƒ•áƒ˜áƒ¡áƒáƒ£áƒ‘áƒ áƒáƒ— áƒ áƒáƒ–áƒ”áƒ áƒ¡áƒáƒ”áƒ áƒ—áƒáƒ“ áƒ¥áƒáƒ›áƒ¤áƒ”áƒ—áƒ˜áƒ¨áƒ”áƒœáƒ˜:
áƒ›áƒ˜áƒ–áƒáƒœáƒ˜áƒ áƒ©áƒ•áƒ”áƒ£áƒšáƒ”áƒ‘áƒ áƒ˜áƒ•áƒ˜ áƒ¤áƒáƒ¢áƒáƒáƒáƒáƒ áƒáƒ¢áƒ˜áƒ— áƒ’áƒáƒ“áƒáƒ¦áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ¡áƒ£áƒ áƒáƒ—áƒ”áƒ‘áƒ˜, áƒ›áƒáƒœáƒ”áƒ¡ áƒœáƒáƒ®áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒ¢áƒ˜áƒšáƒ¨áƒ˜ áƒ“áƒáƒ•áƒáƒ’áƒ”áƒœáƒ”áƒ áƒ˜áƒ áƒáƒ—.
photo_jgp/ áƒ¤áƒáƒ¢áƒáƒ”áƒ‘áƒ˜
monet_jpg/ áƒ›áƒáƒœáƒ”áƒ¡ áƒœáƒáƒ®áƒáƒ¢áƒ”áƒ‘áƒ˜

áƒ¡áƒáƒ¥áƒ›áƒ” áƒ˜áƒ›áƒáƒ¨áƒ˜áƒ áƒ áƒáƒ› áƒ©áƒ•áƒ”áƒœ áƒáƒ  áƒ’áƒ•áƒáƒ¥áƒ•áƒ¡ áƒ›áƒáƒœáƒ”áƒ¡ áƒœáƒáƒ®áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒ¡áƒáƒ‘áƒ˜áƒ›áƒ˜áƒ¡áƒ˜ áƒ¡áƒ£áƒ áƒáƒ—áƒ”áƒ‘áƒ˜, áƒáƒ›áƒ˜áƒ¢áƒáƒ› supervised learning áƒáƒ  áƒ’áƒáƒ›áƒáƒ’áƒ•áƒ˜áƒ•áƒ.

áƒ áƒáƒ¢áƒáƒ› áƒ•áƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ— cycleGAN-áƒ¡: 
áƒ”áƒ¡ áƒ›áƒáƒ“áƒ”áƒšáƒ˜ áƒáƒ’áƒ•áƒáƒ áƒ”áƒ‘áƒ¡ áƒ˜áƒ› áƒáƒ áƒáƒ‘áƒšáƒ”áƒ›áƒáƒ¡ áƒ áƒ áƒ¡áƒ£áƒ áƒáƒ—áƒ”áƒ‘áƒ˜áƒ¡ áƒ¬áƒ§áƒ•áƒ˜áƒšáƒ”áƒ‘áƒ˜ áƒáƒ áƒ’áƒ•áƒáƒ¥áƒ•áƒ¡, áƒáƒ áƒ›áƒáƒ’áƒ˜ áƒ›áƒ˜áƒ›áƒáƒ áƒ—áƒ£áƒšáƒ”áƒ‘áƒ˜áƒ— áƒ¡áƒ¬áƒáƒ•áƒšáƒ”áƒ‘áƒ˜áƒ—!
G_P2M áƒ¤áƒáƒ¢áƒ -> áƒ›áƒáƒœáƒ”
G_M2P áƒ›áƒáƒœáƒ” -> áƒ¤áƒáƒ¢áƒ (áƒ”áƒ¡ áƒ“áƒáƒ›áƒ®áƒ›áƒáƒ áƒ”áƒ áƒ£áƒ¤áƒ áƒ, áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ˜áƒ¡ áƒ¡áƒ¢áƒáƒ‘áƒ˜áƒšáƒ£áƒ áƒáƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡)

áƒ’áƒáƒ›áƒáƒ•áƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ— 2 áƒ“áƒ˜áƒ¡áƒ™áƒ áƒ˜áƒ›áƒ˜áƒœáƒáƒ¢áƒáƒ áƒ¡: D_M(áƒáƒ¤áƒáƒ¡áƒ”áƒ‘áƒ¡ áƒ›áƒ˜áƒ¦áƒ”áƒ‘áƒ£áƒš áƒ›áƒáƒœáƒ”áƒ¡ áƒœáƒáƒ®áƒáƒ¢áƒ¡) áƒ“áƒ  D_P(áƒáƒ¤áƒáƒ¡áƒ”áƒ‘áƒ¡ áƒ›áƒ˜áƒ¦áƒ”áƒ‘áƒ£áƒš áƒ¤áƒáƒ¢áƒáƒ¡). 

CycleGAN áƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ¡ cycle-consistency-áƒ˜áƒ¡:
1. áƒáƒ•áƒ˜áƒ¦áƒáƒ— áƒ áƒ”áƒáƒšáƒ£áƒ áƒ˜ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜ P.
2. áƒ’áƒáƒ“áƒáƒ•áƒ˜áƒ§áƒ•áƒáƒœáƒáƒ— áƒ›áƒáƒœáƒ”áƒ¨áƒ˜: P â†’ G_P2M(P) = fake_M
3. áƒ’áƒáƒ“áƒ›áƒáƒ•áƒ˜áƒ§áƒ•áƒáƒœáƒáƒ— áƒ£áƒ™áƒáƒœ áƒ¤áƒáƒ¢áƒáƒ¨áƒ˜: fake_M â†’ G_M2P(fake_M) = rec_P
4. áƒ•áƒ”áƒªáƒáƒ“áƒáƒ— áƒ áƒáƒ› rec_P áƒ“áƒ P áƒ’áƒáƒ•áƒ“áƒœáƒ”áƒœ áƒ”áƒ áƒ—áƒ›áƒáƒœáƒ”áƒ—áƒ¡.

áƒšáƒáƒ’áƒ˜áƒ™áƒ áƒ˜áƒ¡ áƒ“áƒ”áƒ•áƒ¡, áƒ áƒáƒ› áƒáƒ› áƒ›áƒ”áƒ—áƒáƒ“áƒ˜áƒ— áƒ›áƒáƒ“áƒ”áƒšáƒ˜ áƒ£áƒ¤áƒ áƒ áƒ‘áƒ”áƒ•áƒ áƒ¡ áƒ˜áƒ¡áƒ¬áƒáƒ•áƒšáƒ˜áƒ¡ áƒ áƒáƒ’áƒáƒ áƒª áƒ¡áƒ£áƒ áƒáƒ—áƒ–áƒ” áƒáƒ¡áƒ”áƒ•áƒ” áƒ›áƒáƒœáƒ”áƒ–áƒ”.

### áƒáƒ˜áƒ áƒ•áƒ”áƒšáƒ˜ áƒ”áƒ¥áƒ¡áƒáƒ”áƒ áƒ˜áƒ›áƒ”áƒœáƒ¢áƒ˜: U-Net Generator vs ResNet Generator.

1. áƒ’áƒ”áƒœáƒ”áƒ áƒáƒ¢áƒáƒ áƒ˜áƒ¡ áƒ áƒáƒšáƒ˜:

CycleGAN-áƒ¨áƒ˜ áƒ’áƒ”áƒœáƒ”áƒ áƒáƒ¢áƒáƒ áƒ¡ áƒ’áƒáƒ“áƒáƒ§áƒáƒ•áƒ¡ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜ áƒ”áƒ áƒ—áƒ˜ áƒ“áƒáƒ›áƒ”áƒœáƒ˜áƒ“áƒáƒœ áƒ›áƒ”áƒáƒ áƒ”áƒ¨áƒ˜.
áƒ˜áƒœáƒáƒ®áƒáƒ•áƒ¡ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒáƒ¡ áƒ“áƒ áƒ¨áƒ˜áƒœáƒáƒáƒ áƒ¡áƒ¡.
áƒ¡áƒ¢áƒ˜áƒšáƒ˜áƒ¡, áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ£áƒ áƒ˜áƒ¡áƒ áƒ“áƒ áƒ¤áƒ”áƒ áƒ˜áƒ¡ áƒ’áƒáƒœáƒáƒ¬áƒ˜áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ•áƒšáƒ.
áƒªáƒ˜áƒ™áƒšáƒ˜áƒ¡ áƒ—áƒáƒœáƒ›áƒ˜áƒ›áƒ“áƒ”áƒ•áƒ áƒ£áƒšáƒáƒ‘áƒ, áƒáƒœáƒ£ áƒáƒ áƒ˜áƒ’áƒ˜áƒœáƒáƒšáƒ£áƒ  áƒ’áƒáƒ›áƒáƒ¡áƒáƒ®áƒ£áƒšáƒ”áƒ‘áƒáƒ–áƒ” áƒ˜áƒœáƒ•áƒ”áƒ áƒ¡áƒ˜áƒ£áƒšáƒ˜ áƒ¨áƒ”áƒ¡áƒáƒ‘áƒáƒ›áƒ˜áƒ¡áƒáƒ‘áƒ˜áƒ¡ áƒáƒáƒ•áƒœáƒ.

2. ResNet Generator

áƒáƒ¥ áƒ•áƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘ áƒ˜áƒ› áƒáƒ áƒ¥áƒ˜áƒ¢áƒ”áƒ¥áƒ¢áƒ£áƒ áƒáƒ¡ áƒ áƒáƒª CycleGan-áƒ˜áƒ¡ áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒáƒªáƒ˜áƒáƒ¨áƒ˜ áƒ’áƒ•áƒ®áƒ•áƒ“áƒ”áƒ‘áƒ.
Initial convolution with large receptive field
Downsampling using strided convolutions
Multiple residual blocks
Upsampling back to original resolution
Final Tanh output layer

The core building block is the residual block:

y=x+F(x), where F(x) represents a small convolutional transformation.


3. U-Net Generator
The U-Net generator follows an encoderâ€“decoder structure with long skip connections between corresponding layers:

Encoder progressively downsamples the image

Bottleneck captures global representation

Decoder upsamples back to full resolution

Skip connections concatenate encoder features to decoder layers

Unlike ResNet, U-Net connects early low-level features directly to late reconstruction layers.


## next_Monet_Lodia_exp1_v2_full.ipynb (áƒ”áƒ¡ áƒáƒ áƒ˜áƒ¡ áƒ“áƒáƒ¡áƒ áƒ£áƒšáƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ¤áƒáƒ˜áƒšáƒ˜, áƒ™áƒ˜áƒ“áƒ”áƒ• áƒ áƒáƒ›áƒ“áƒ”áƒœáƒ˜áƒ›áƒ” áƒ¤áƒáƒ˜áƒšáƒ˜áƒ áƒ áƒáƒ›áƒšáƒ”áƒ‘áƒ˜áƒª áƒ‘áƒáƒšáƒáƒ›áƒ“áƒ” áƒ•áƒ”áƒ  áƒ’áƒáƒ•áƒ˜áƒ“áƒœáƒ”áƒœ GPU-áƒ¡ áƒ’áƒáƒ›áƒ, áƒ›áƒáƒ’áƒ áƒáƒ› áƒ©áƒ”áƒ¥áƒáƒáƒ˜áƒœáƒ¢áƒ”áƒ‘áƒ˜áƒ— áƒ•áƒáƒ’áƒ áƒ«áƒ”áƒšáƒ”áƒ‘áƒ“áƒ˜)
This code is:
CycleGAN with ResNet generator + PatchGAN discriminator + LSGAN loss at 256Ã—256.


# áƒ¯áƒ”áƒ  áƒ•áƒ˜áƒ¡áƒáƒ£áƒ‘áƒ áƒáƒ— áƒ“áƒáƒ¢áƒáƒ–áƒ” áƒ“áƒ preprocessing-áƒ–áƒ”:

áƒ“áƒáƒ¢áƒ:áƒ“áƒáƒ•áƒ¬áƒ”áƒ áƒ” MonetPhotoDatase, áƒ áƒáƒ› áƒ§áƒáƒ•áƒ”áƒš áƒ¡áƒ”áƒ›áƒáƒšáƒ–áƒ” áƒ›áƒáƒ›áƒªáƒ”áƒ¡ áƒ”áƒ áƒ—áƒ˜ áƒ¤áƒáƒ¢áƒ áƒ“áƒ áƒ”áƒ áƒ—áƒ˜ áƒœáƒáƒ®áƒáƒ¢áƒ˜.
áƒ§áƒáƒ•áƒ”áƒš áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ˜áƒ¡ áƒœáƒáƒ‘áƒ˜áƒ¯áƒ–áƒ” áƒ’áƒ•áƒ”áƒ¥áƒœáƒ”áƒ‘áƒ áƒœáƒáƒ›áƒ“áƒ•áƒ˜áƒšáƒ˜ áƒ¤áƒáƒ¢áƒ real_P áƒ“áƒ áƒ›áƒáƒœáƒ”áƒ¡ áƒœáƒáƒ®áƒáƒ¢áƒ˜ real_M. áƒ”áƒ¡áƒ”áƒœáƒ˜ áƒ¬áƒ§áƒ•áƒ˜áƒšáƒ˜ áƒáƒ  áƒáƒ áƒ˜, áƒ¤áƒ áƒ˜áƒáƒ“ áƒ“áƒáƒ›áƒáƒ£áƒ™áƒ˜áƒ“áƒ”áƒ‘áƒšáƒ”áƒ‘áƒ˜ áƒáƒ áƒ˜áƒáƒœ.
preprocessing: áƒ•áƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ— - Resize(256,256), RandomHorizontalFlip(0.5) (simple augmentation),ToTensor()Normalize(mean=0.5, std=0.5) per RGB channel
This normalization maps pixel range [0,1] to [-1,1], which matches the generatorâ€™s final Tanh() output.

batch_sizeáƒ¡ áƒ•áƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘ áƒ”áƒ áƒ—áƒ¡ áƒ áƒáƒ“áƒ’áƒáƒœ áƒáƒ¤áƒ˜áƒªáƒ˜áƒáƒšáƒ£áƒ  áƒ¤áƒ”áƒ˜áƒ¤áƒ”áƒ áƒ¨áƒ˜ áƒ”áƒ’áƒ áƒ” áƒ”áƒ¬áƒ”áƒ áƒ. áƒáƒ›áƒªáƒ˜áƒ áƒ”áƒ‘áƒ¡ gpuáƒ¡ áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒáƒ¡ 256x256áƒ–áƒ”.

# áƒáƒ  áƒ’áƒáƒ›áƒáƒ›áƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ˜áƒ áƒáƒ áƒª áƒ”áƒ áƒ—áƒ˜ áƒ¨áƒ”áƒ–áƒ¦áƒ£áƒ“áƒ£áƒšáƒ˜ tool.
áƒ“áƒáƒ•áƒ¬áƒ”áƒ áƒ”: ResNetGenerator(nn.Module), PatchDiscriminator(nn.Module), training loop (manual), loss formulas (explicit).

# ResNet-based Generator:
ngf = 32 (lighter/faster than the common 64)
n_blocks = 9 (recommended for 256Ã—256 in CycleGAN)
n_downsampling = 2

áƒ©áƒ•áƒ”áƒœáƒ˜ áƒ’áƒ”áƒœáƒ”áƒ áƒáƒ¢áƒáƒ áƒ˜ áƒáƒ áƒ˜áƒ¡ encoderâ€“transformerâ€“decoder.
1. Initial conv (7Ã—7)
  Reflection padding to avoid edge artifacts
  Conv â†’ InstanceNorm â†’ ReLU

2.Downsampling Ã—2
  Each downsampling halves resolution and increases channels:
  256Ã—256 â†’ 128Ã—128 â†’ 64Ã—64
  channels: 32 â†’ 64 â†’ 128
  
3.Residual blocks (9 blocks) at 64Ã—64
  Each block:
    does Conv â†’ INorm â†’ ReLU â†’ Conv â†’ INorm
    then adds skip connection: x + block(x)
    These blocks keep spatial size but learn style/content transformations.

4.Upsampling Ã—2 (ConvTranspose)
    64Ã—64 â†’ 128Ã—128 â†’ 256Ã—256
    channels go back down: 128 â†’ 64 â†’ 32

5.Output conv (7Ã—7) + Tanh
    output has 3 channels (RGB)
    output range is [-1, 1]

# Discriminator: PatchGAN(70x70)
áƒ”áƒ¡ áƒ“áƒ˜áƒ¡áƒ™áƒ áƒ˜áƒ›áƒ˜áƒœáƒáƒ¢áƒáƒ áƒ˜ áƒáƒ  áƒ’áƒ•áƒ˜áƒ‘áƒ áƒ£áƒœáƒ”áƒ‘áƒ¡ áƒáƒáƒ¡áƒ£áƒ®áƒáƒ“ áƒ£áƒ‘áƒ áƒáƒšáƒáƒ“ real/fake-áƒ¡. áƒ’áƒ•áƒ˜áƒ‘áƒ áƒ£áƒœáƒ”áƒ‘áƒ¡ score-áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒ áƒ˜áƒ“áƒ¡. áƒ¡áƒ®áƒ•áƒáƒœáƒáƒ˜áƒ áƒáƒ“ áƒ áƒáƒ› áƒáƒ•áƒ®áƒ¡áƒœáƒ áƒ˜áƒ¡ áƒáƒ  áƒ›áƒáƒáƒ¡áƒ£áƒ®áƒáƒ‘áƒ¡ áƒ”áƒ¡ áƒáƒ áƒ˜áƒ¡ áƒ—áƒ£ áƒáƒ áƒ áƒ›áƒáƒœáƒ”áƒ¡ áƒœáƒáƒ®áƒáƒ¢áƒ˜, áƒ˜áƒ¡ áƒ›áƒ”áƒ£áƒ‘áƒœáƒ”áƒ‘áƒ áƒ™áƒáƒœáƒ™áƒ áƒ”áƒ¢áƒ£áƒšáƒ˜ áƒœáƒáƒ¬áƒ˜áƒšáƒ”áƒ‘áƒ˜ áƒ¡áƒ£áƒ áƒáƒ—áƒ”áƒ‘áƒ˜áƒ¡ áƒáƒ áƒ˜áƒ¡ áƒ—áƒ£ áƒáƒ áƒ áƒ›áƒáƒœáƒ”áƒ¡ áƒ¡áƒ¢áƒ˜áƒšáƒ˜áƒ¡.
áƒáƒ áƒ¥áƒ˜áƒ¢áƒ”áƒ¥áƒ¢áƒ£áƒ áƒ: 
We use 4Ã—4 convolutions:
    First layer: Conv(3â†’32), stride 2, LeakyReLU
    Next layers: Conv with InstanceNorm + LeakyReLU
    Final layer outputs 1-channel map of realism scores

áƒ’áƒ•áƒ˜áƒ‘áƒ áƒ£áƒœáƒ”áƒ‘áƒ¡ áƒ”áƒ’áƒ áƒ”áƒ“ áƒ¬áƒáƒ“áƒ”áƒ‘áƒ£áƒš patch map-áƒ¡. áƒ›áƒáƒ¦áƒáƒšáƒ˜ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜áƒ áƒ¡áƒáƒ“áƒáƒª áƒ’áƒáƒ•áƒ¡, áƒ“áƒáƒ‘áƒáƒšáƒ˜áƒ áƒ¡áƒáƒ“áƒáƒª áƒáƒ  áƒ’áƒáƒ•áƒ¡.

# Loss Funcs

áƒ•áƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘ áƒ¡áƒáƒ›áƒœáƒáƒ˜áƒ  loss-áƒ¡.
1. Adversarial loss(LSGAN)
   criterion_GAN = nn.MSELoss()

2. Cycle-consistency loss
   "áƒáƒ™áƒáƒœáƒ¢áƒ áƒáƒšáƒ”áƒ‘áƒ¡ áƒ’áƒáƒ“áƒáƒ¡áƒ•áƒšáƒ”áƒ‘áƒ¡":
     P â†’ M â†’ P should reconstruct original P
     M â†’ P â†’ M should reconstruct original M
     criterion_cycle = nn.L1Loss()

3. Identity loss
   áƒ•áƒªáƒ“áƒ˜áƒšáƒáƒ‘áƒ— áƒáƒ  áƒ“áƒáƒ•áƒ£áƒ¨áƒ•áƒáƒ— áƒáƒ áƒáƒ¡áƒ˜áƒ­áƒ˜áƒ áƒ áƒ¤áƒ”áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒªáƒ•áƒšáƒ. 
   áƒ©áƒ•áƒ”áƒœ áƒ áƒáƒ› áƒ›áƒáƒœáƒ”áƒ¡ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜ áƒ’áƒáƒ•áƒ£áƒ¨áƒ•áƒáƒ— áƒ›áƒáƒœáƒ”->áƒ¡áƒ£áƒ áƒáƒ—áƒ˜ áƒ’áƒ”áƒœáƒ”áƒ áƒáƒ¢áƒáƒ áƒ¨áƒ˜, áƒ–áƒ£áƒ¡áƒ¢áƒáƒ“ áƒ˜áƒ’áƒ˜áƒ•áƒ” áƒ£áƒœáƒ“áƒ áƒ“áƒáƒ’áƒ•áƒ˜áƒ‘áƒ áƒ£áƒœáƒáƒ¡
     G_P2M(M) â‰ˆ M, G_M2P(P) â‰ˆ P
     criterion_identity = nn.L1Loss()


# áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ˜

  áƒ’áƒ”áƒœáƒ”áƒ áƒáƒ¢áƒáƒ áƒ˜áƒ¡ áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ˜
  1. identity loss
  2. adversarial loss for Photoâ†’Monet
  3. adversarial loss for Monetâ†’Photo
  4. cycle losses for both cycles
    Then sum them and backprop once for both generators. his is why our generator loss is called G_total.

  áƒ›áƒáƒœáƒ”áƒ¡ áƒ“áƒ˜áƒ¡áƒ™áƒ áƒ˜áƒ›áƒ˜áƒœáƒáƒ¢áƒáƒ áƒ˜áƒ¡ áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ˜:
  loss_real: D_M(real_M) â†’ 1
  loss_fake: D_M(fake_M.detach()) â†’ 0
  Then average.

  áƒ¡áƒ£áƒ áƒáƒ—áƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒ˜áƒ¡áƒ™áƒ áƒ˜áƒ›áƒ˜áƒœáƒáƒ¢áƒáƒ áƒ˜áƒ¡ áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’áƒ˜:
  D_P(real_P) â†’ 1
  D_P(fake_P.detach()) â†’ 0
  Then average.

# Run Results:
  D_M = 0.01413
  D_P = 0.25527
  G_M2P_adv = 0.53436
  G_P2M_adv = 1.17031
  G_total = 5.50306
  cycle_loss = 2.58305
  idt_M = 0.45792
  idt_P = 0.75742

  áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜áƒ“áƒáƒœ áƒ•áƒ®áƒ”áƒ“áƒáƒ•áƒ—, áƒ áƒáƒ› D_M = 0.014, áƒ áƒáƒª áƒ«áƒáƒšáƒ˜áƒáƒœ áƒ“áƒáƒ‘áƒáƒšáƒ˜ áƒ“áƒ áƒªáƒ£áƒ“áƒ˜ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜áƒ. áƒœáƒ˜áƒ¨áƒœáƒáƒ•áƒ¡, áƒ áƒáƒ› áƒ“áƒ˜áƒ¡áƒ™áƒ áƒ˜áƒ›áƒáƒœáƒáƒ¢áƒáƒ áƒ˜ áƒ›áƒáƒ áƒ¢áƒ˜áƒ•áƒáƒ“ áƒáƒ áƒ©áƒ”áƒ•áƒ¡ áƒ¡áƒ¬áƒáƒ  áƒ“áƒ áƒ§áƒáƒšáƒ‘ áƒœáƒáƒ®áƒáƒ¢áƒ”áƒ‘áƒ¡. áƒ’áƒ”áƒœáƒ”áƒ áƒáƒ¢áƒáƒ áƒ˜ áƒ›áƒáƒ áƒªáƒ®áƒ“áƒ”áƒ‘áƒ. D_P = 0.255 áƒœáƒ˜áƒ¨áƒœáƒáƒ•áƒ¡ áƒ áƒáƒ› áƒ›áƒáƒœáƒ”->áƒ¤áƒáƒ¢áƒ áƒ£áƒ¤áƒ áƒ áƒ áƒ”áƒáƒšáƒ˜áƒ¡áƒ¢áƒ£áƒ áƒ˜ áƒ’áƒáƒ›áƒáƒ•áƒ˜áƒ“áƒ.
  G_M2P_adv = 0.53436
  G_P2M_adv = 1.17031
  áƒ”áƒ¡ áƒœáƒáƒ¬áƒ˜áƒšáƒ˜ áƒáƒ›áƒ§áƒáƒ áƒ”áƒ‘áƒ¡ áƒ–áƒ”áƒ›áƒáƒ— áƒœáƒáƒ—áƒ¥áƒ•áƒáƒ›áƒ¡. D_M áƒ«áƒšáƒ˜áƒ”áƒ áƒ˜áƒ.
  cycle_loss = 2.58305 áƒ”áƒ¡áƒ”áƒª áƒ«áƒáƒšáƒ˜áƒáƒœ áƒ›áƒáƒ¦áƒáƒšáƒ˜áƒ áƒ“áƒ áƒœáƒ˜áƒ¨áƒœáƒáƒ•áƒ¡ áƒ áƒáƒ› áƒ¡áƒ£áƒ áƒáƒ—áƒ”áƒ‘áƒ˜áƒ¡ áƒªáƒ˜áƒ™áƒšáƒ˜ áƒ’áƒáƒœáƒ¡áƒ®áƒ•áƒáƒ•áƒ“áƒ”áƒ‘áƒ áƒ¡áƒáƒ¬áƒ§áƒ¡áƒ˜áƒ¡áƒ’áƒáƒœ.
  idt_M = 0.45792
  idt_P = 0.75742
  áƒ”áƒ¡áƒ”áƒœáƒ˜ áƒœáƒ˜áƒ¨áƒœáƒáƒ•áƒ¡ áƒ áƒáƒ› áƒ’áƒ”áƒœáƒ”áƒ áƒáƒ¢áƒáƒ áƒ˜ áƒ˜áƒ›áƒáƒ–áƒ” áƒ£áƒ¤áƒ áƒ áƒªáƒ•áƒšáƒ˜áƒ¡ áƒœáƒáƒ®áƒáƒ¢áƒ”áƒ‘áƒ¡ áƒ•áƒ˜áƒ“áƒ áƒ” áƒ¡áƒáƒ­áƒ˜áƒ áƒáƒ.


## next_Monet_Lodia_U_Net_exp1.ipynb

FID: 93.3667451163031
MiFID: 93.3667451163031

# áƒ¨áƒ”áƒ•áƒªáƒ•áƒáƒšáƒ”áƒ— ResNet -> U-Net
áƒáƒ› áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ˜áƒ£áƒ®áƒ”áƒ“áƒáƒ•áƒáƒ“, áƒ˜áƒ“áƒ”áƒœáƒ¢áƒ£áƒ áƒ˜ áƒ“áƒáƒ áƒ©áƒ:
1. áƒ“áƒ˜áƒ¡áƒ™áƒ áƒ˜áƒ›áƒ˜áƒœáƒáƒ¢áƒáƒ áƒ˜
2. LSGAN, cycle-consistency, identity
3. áƒ“áƒáƒ¢áƒ áƒ“áƒ preprocessing
4. 256x256

áƒ›áƒ˜áƒ–áƒáƒœáƒ˜áƒ áƒ’áƒáƒ•áƒáƒ áƒ™áƒ•áƒ˜áƒáƒ— áƒ áƒáƒ’áƒáƒ  áƒ¨áƒ”áƒ˜áƒªáƒ•áƒšáƒ”áƒ‘áƒ Cycle-consistency, Training stability...

U-Net architectures are known to be effective when low-level spatial information must be preserved, which is particularly relevant for artistic style transfer.

# U-Net Generator Architecture
  The U-Net generator follows an encoderâ€“decoder structure with skip connections, where feature maps from the downsampling path are concatenated with corresponding upsampling layers.

1. Downsampling path:
    Series of stride-2 convolutions
    Gradually increases channel depth
    Captures global context and semantics

2. Upsampling path:
     Transposed convolutions
     Gradually restores spatial resolution

3. Skip connections:
    Direct concatenation of encoder features into decoder
    Preserve fine spatial details (edges, textures)

4. Final activation:
     Tanh, producing outputs in [âˆ’1,1]

Unlike the ResNet generator, U-Net does not rely on residual blocks for information flow; instead, it explicitly reuses early-layer features, which has a strong impact on visual sharpness.

| Aspect             | ResNet Generator     | U-Net Generator             |
| ------------------ | -------------------- | --------------------------- |
| Core idea          | Residual learning    | Encoderâ€“decoder + skips     |
| Feature reuse      | Implicit (residuals) | Explicit (skip connections) |
| Spatial detail     | Moderately preserved | Strongly preserved          |
| Texture sharpness  | Softer               | Sharper                     |
| Memory usage       | Lower                | Higher                      |
| Training stability | Very stable          | Slightly more oscillatory   |

# Run Results

For U-Net:

| Metric       | Value     |        
| ------------ | --------- |
| G_total    | **2.72**  |
| G_P2M_adv  | 1.12      |
| G_M2P_adv  | 0.67      |
| cycle_loss | **0.75**  |
| idt_M      | **0.07**  |
| idt_P      | **0.11**  |
| D_M        | 0.017     |
| D_P       | **0.036** |

For ResNet:

D_M = 0.01413
D_P = 0.25527
G_M2P_adv = 0.53436
G_P2M_adv = 1.17031
G_total = 5.50306
cycle_loss = 2.58305
idt_M = 0.45792
idt_P = 0.75742

Cycle loss is significantly lower than in the ResNet experiment
â†’ U-Net reconstructs images more faithfully.

Identity losses are much smaller
â†’ The generator minimally alters images already in the target domain.

Discriminator losses are low but balanced
â†’ No discriminator collapse observed.

Overall generator loss is lower than ResNet
â†’ Training objective is easier for U-Net due to skip connections.


13.1 Photo â†’ Monet Translation

Observed characteristics:

Strong Monet-style color palettes

Clear brush-like textures

Better preservation of edges and object boundaries

Less excessive blurring compared to ResNet

U-Netâ€™s skip connections help retain local structure, which results in sharper stylization.

13.2 Cycle Consistency (P â†’ M â†’ P, M â†’ P â†’ M)

Cycle reconstructions show:

High structural fidelity

Minimal geometric distortion

Very small color drift

This aligns with the low cycle loss (0.75) and confirms that U-Net is particularly effective at reconstruction-based constraints.

13.3 Monet â†’ Photo Translation

Generated photos are sharper than ResNet outputs

Some painterly artifacts remain (expected)

Slightly reduced realism compared to ResNet in certain scenes

This highlights a trade-off:

U-Net prioritizes structure

ResNet slightly prioritizes global realism


---------------------------------------------------------------------------------------
16.1 Summary of Experimental Results

The experiments demonstrated that both architectures successfully learned meaningful mappings between the photo and Monet domains without paired supervision.

The ResNet-based CycleGAN produced visually smooth and globally consistent Monet-style images, capturing color palettes and painterly textures effectively.

The U-Net-based CycleGAN achieved stronger cycle consistency and identity preservation, producing sharper images with clearer structural details.

Quantitatively, the U-Net model achieved:

Lower cycle-consistency loss

Lower identity loss

Lower overall generator loss

While the ResNet model showed:

Slightly better global stylization

More stable adversarial dynamics

These results indicate that architecture choice directly affects the trade-off between stylistic abstraction and structural fidelity.

16.2 What We Learned About CycleGAN

Through this project, several key insights about CycleGAN were observed:

Cycle-consistency is essential
Without the cycle loss, the generators would easily collapse to arbitrary mappings. The low cycle loss values in both experiments confirm that the bidirectional constraint is doing meaningful work.

Identity loss improves color stability
Identity loss helped prevent unnecessary color shifts when an image was already in the target domain, especially noticeable in the U-Net experiment.

Unpaired training is feasible but fragile
Training without paired data works, but requires careful balancing of losses and learning rates to avoid mode collapse or texture artifacts.

Discriminator loss alone is not a quality metric
Very low discriminator loss (e.g., D_M â‰ˆ 0.01) does not necessarily imply better images; visual inspection remains critical.

16.3 Architectural Insights: ResNet vs U-Net

This experiment highlighted how architectural inductive biases shape the learned mapping:

ResNet generators rely on residual learning to modify features gradually, leading to smoother and more painterly outputs.

U-Net generators reuse low-level features via skip connections, which preserves edges and spatial structure but can reduce stylistic abstraction.

In other words:

ResNet favors artistic transformation

U-Net favors structural reconstruction

Neither architecture is universally better; the â€œbestâ€ choice depends on whether the task prioritizes style realism or content fidelity.

16.4 Practical Takeaways

From an engineering perspective, the project provided practical lessons:

Batch size = 1 is critical for CycleGAN stability.

Training time scales heavily with image resolution.

Checkpointing is essential due to long training times and unstable runtimes.

Visual monitoring is necessary; losses alone are insufficient.

16.5 Final Remarks

Overall, this project demonstrates that CycleGAN is a powerful framework for unpaired image translation, capable of learning complex artistic transformations from limited supervision. The comparative study between ResNet and U-Net generators shows that model architecture plays a crucial role in balancing realism, structure, and artistic style.

------------------------------------------------------------------------------------------------------------











LSGAN vs Hinge Loss

FID: 89.65889200145094
MiFID: 103.301895

In this experiment, we compare two different adversarial loss formulations used to train Generative Adversarial Networks (GANs): Least Squares GAN (LSGAN) and Hinge Adversarial Loss.
Both losses define how the generator and discriminator compete during training, but they differ in how strongly and in what regime gradients are applied.

1. Background: Adversarial Loss in GANs

A GAN consists of two networks:

Generator (G): tries to generate fake images that look real

Discriminator (D): tries to distinguish real images from fake ones

The adversarial loss defines:

What it means for the discriminator to be â€œcorrectâ€

How the generator is rewarded or penalized based on discriminator feedback

Different loss functions lead to different training dynamics, stability, and image quality.

2. Least Squares GAN (LSGAN)
Motivation

The original GAN loss (binary cross-entropy) often suffers from:

Vanishing gradients

Unstable training

Saturation when the discriminator becomes too confident

LSGAN was proposed to address these issues by replacing binary classification with a least-squares regression objective.

LSGAN Discriminator Loss

For real images xâˆ¼Pdata:
	â€‹
ğ¿ğ·ğ‘Ÿğ‘’ğ‘ğ‘™ =(ğ·(ğ‘¥)âˆ’1)2 (2 means square)

For fake images G(z): LDfake = (D(G(z)))square

Total discriminator loss = 1/2[LDreal + LDfake]

LSGAN Generator Loss

The generator tries to make fake images look real:


LG=(D(G(z))âˆ’1)square

3. Hinge Adversarial Loss
Motivation

Hinge loss comes from margin-based classification, commonly used in SVMs.
Instead of penalizing all mistakes equally, it enforces a margin:

If the discriminator is already confident enough â†’ no loss

Focuses learning on hard examples

This often leads to:

Stronger discriminators

Sharper images

More stable gradients in practice

Hinge Discriminator Loss

For real images:
LDreal=max(0,1âˆ’D(x))

For fake images:
LDfake=max(0,1+D(G(z)))

total loss: E[LDreal + Ldfake]

Hinge Generator Loss

The generator simply tries to increase the discriminator score:


LG=âˆ’E[D(G(z))]
